{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /var/data/python/lib/python3.11/site-packages (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /var/data/python/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /var/data/python/lib/python3.11/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /var/data/python/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /var/data/python/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /var/data/python/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /var/data/python/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /var/data/python/lib/python3.11/site-packages (from scikit-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /var/data/python/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /var/data/python/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /var/data/python/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /var/data/python/lib/python3.11/site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /var/data/python/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neural_network.layers.dense_layer import DenseLayer\n",
    "from neural_network.layers.dropout_layer import DropoutLayer\n",
    "from neural_network.optimizer import GradientDescent\n",
    "from neural_network.losses import CrossEntropyLoss,MeanSquaredError\n",
    "from neural_network.utils import normalize_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import ( accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, mean_absolute_error,\n",
    "    mean_squared_error, r2_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('../data/regression/student-por.csv')\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "label_encoders = {col: LabelEncoder() for col in categorical_cols}\n",
    "for col in categorical_cols:\n",
    "    df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "df['absences'] = df['absences'].clip(upper=20)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "X = df.drop(columns=['G3'])\n",
    "y = df['G3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss, optimizer):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        grad = self.loss.backward(y_true, y_pred)\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "    \n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            params = layer.get_parameters()\n",
    "            if params:\n",
    "                grads = layer.get_gradients()\n",
    "                weights_updated, biases_updated = self.optimizer.update(\n",
    "                    params['weights'], params['biases'],\n",
    "                    grads['weights'], grads['biases']\n",
    "                )\n",
    "                layer.set_parameters({'weights': weights_updated, 'biases': biases_updated})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "\n",
    "layers = [\n",
    "    DenseLayer(input_size=input_size, output_size=128, activation='relu'), \n",
    "    DropoutLayer(rate=0.2), \n",
    "    DenseLayer(input_size=128, output_size=64, activation='relu'),  \n",
    "    DenseLayer(input_size=64, output_size=32, activation='linear'), \n",
    "\n",
    "    DenseLayer(input_size=32, output_size=1, activation='linear')  \n",
    "]\n",
    "\n",
    "loss = MeanSquaredError()\n",
    "\n",
    "optimizer = GradientDescent(learning_rate=0.01)  \n",
    "model = NeuralNetwork(layers=layers, loss=loss, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0010 | Loss: 1.1355 | MAE: 0.8020\n",
      "Epoch 0020 | Loss: 1.0703 | MAE: 0.7774\n",
      "Epoch 0030 | Loss: 1.0228 | MAE: 0.7594\n",
      "Epoch 0040 | Loss: 0.9841 | MAE: 0.7444\n",
      "Epoch 0050 | Loss: 0.9509 | MAE: 0.7313\n",
      "Epoch 0060 | Loss: 0.9219 | MAE: 0.7195\n",
      "Epoch 0070 | Loss: 0.8956 | MAE: 0.7086\n",
      "Epoch 0080 | Loss: 0.8708 | MAE: 0.6984\n",
      "Epoch 0090 | Loss: 0.8473 | MAE: 0.6889\n",
      "Epoch 0100 | Loss: 0.8248 | MAE: 0.6797\n",
      "Epoch 0110 | Loss: 0.8028 | MAE: 0.6705\n",
      "Epoch 0120 | Loss: 0.7817 | MAE: 0.6617\n",
      "Epoch 0130 | Loss: 0.7613 | MAE: 0.6530\n",
      "Epoch 0140 | Loss: 0.7417 | MAE: 0.6448\n",
      "Epoch 0150 | Loss: 0.7225 | MAE: 0.6367\n",
      "Epoch 0160 | Loss: 0.7039 | MAE: 0.6288\n",
      "Epoch 0170 | Loss: 0.6860 | MAE: 0.6210\n",
      "Epoch 0180 | Loss: 0.6686 | MAE: 0.6132\n",
      "Epoch 0190 | Loss: 0.6513 | MAE: 0.6054\n",
      "Epoch 0200 | Loss: 0.6348 | MAE: 0.5976\n",
      "Epoch 0210 | Loss: 0.6187 | MAE: 0.5897\n",
      "Epoch 0220 | Loss: 0.6030 | MAE: 0.5818\n",
      "Epoch 0230 | Loss: 0.5879 | MAE: 0.5741\n",
      "Epoch 0240 | Loss: 0.5734 | MAE: 0.5665\n",
      "Epoch 0250 | Loss: 0.5589 | MAE: 0.5589\n",
      "Epoch 0260 | Loss: 0.5445 | MAE: 0.5512\n",
      "Epoch 0270 | Loss: 0.5305 | MAE: 0.5437\n",
      "Epoch 0280 | Loss: 0.5173 | MAE: 0.5362\n",
      "Epoch 0290 | Loss: 0.5046 | MAE: 0.5289\n",
      "Epoch 0300 | Loss: 0.4924 | MAE: 0.5219\n",
      "Epoch 0310 | Loss: 0.4808 | MAE: 0.5152\n",
      "Epoch 0320 | Loss: 0.4697 | MAE: 0.5086\n",
      "Epoch 0330 | Loss: 0.4589 | MAE: 0.5022\n",
      "Epoch 0340 | Loss: 0.4484 | MAE: 0.4959\n",
      "Epoch 0350 | Loss: 0.4383 | MAE: 0.4898\n",
      "Epoch 0360 | Loss: 0.4285 | MAE: 0.4840\n",
      "Epoch 0370 | Loss: 0.4190 | MAE: 0.4783\n",
      "Epoch 0380 | Loss: 0.4099 | MAE: 0.4727\n",
      "Epoch 0390 | Loss: 0.4010 | MAE: 0.4672\n",
      "Epoch 0400 | Loss: 0.3924 | MAE: 0.4618\n",
      "Epoch 0410 | Loss: 0.3840 | MAE: 0.4566\n",
      "Epoch 0420 | Loss: 0.3758 | MAE: 0.4514\n",
      "Epoch 0430 | Loss: 0.3678 | MAE: 0.4463\n",
      "Epoch 0440 | Loss: 0.3601 | MAE: 0.4412\n",
      "Epoch 0450 | Loss: 0.3526 | MAE: 0.4363\n",
      "Epoch 0460 | Loss: 0.3452 | MAE: 0.4313\n",
      "Epoch 0470 | Loss: 0.3381 | MAE: 0.4265\n",
      "Epoch 0480 | Loss: 0.3311 | MAE: 0.4217\n",
      "Epoch 0490 | Loss: 0.3242 | MAE: 0.4170\n",
      "Epoch 0500 | Loss: 0.3176 | MAE: 0.4123\n",
      "Epoch 0510 | Loss: 0.3111 | MAE: 0.4078\n",
      "Epoch 0520 | Loss: 0.3047 | MAE: 0.4033\n",
      "Epoch 0530 | Loss: 0.2986 | MAE: 0.3989\n",
      "Epoch 0540 | Loss: 0.2926 | MAE: 0.3946\n",
      "Epoch 0550 | Loss: 0.2867 | MAE: 0.3905\n",
      "Epoch 0560 | Loss: 0.2810 | MAE: 0.3863\n",
      "Epoch 0570 | Loss: 0.2754 | MAE: 0.3822\n",
      "Epoch 0580 | Loss: 0.2700 | MAE: 0.3781\n",
      "Epoch 0590 | Loss: 0.2647 | MAE: 0.3740\n",
      "Epoch 0600 | Loss: 0.2594 | MAE: 0.3699\n",
      "Epoch 0610 | Loss: 0.2544 | MAE: 0.3659\n",
      "Epoch 0620 | Loss: 0.2494 | MAE: 0.3620\n",
      "Epoch 0630 | Loss: 0.2446 | MAE: 0.3580\n",
      "Epoch 0640 | Loss: 0.2398 | MAE: 0.3541\n",
      "Epoch 0650 | Loss: 0.2351 | MAE: 0.3502\n",
      "Epoch 0660 | Loss: 0.2306 | MAE: 0.3465\n",
      "Epoch 0670 | Loss: 0.2261 | MAE: 0.3427\n",
      "Epoch 0680 | Loss: 0.2218 | MAE: 0.3390\n",
      "Epoch 0690 | Loss: 0.2176 | MAE: 0.3354\n",
      "Epoch 0700 | Loss: 0.2135 | MAE: 0.3318\n",
      "Epoch 0710 | Loss: 0.2094 | MAE: 0.3282\n",
      "Epoch 0720 | Loss: 0.2055 | MAE: 0.3247\n",
      "Epoch 0730 | Loss: 0.2016 | MAE: 0.3212\n",
      "Epoch 0740 | Loss: 0.1978 | MAE: 0.3178\n",
      "Epoch 0750 | Loss: 0.1940 | MAE: 0.3145\n",
      "Epoch 0760 | Loss: 0.1903 | MAE: 0.3112\n",
      "Epoch 0770 | Loss: 0.1867 | MAE: 0.3079\n",
      "Epoch 0780 | Loss: 0.1832 | MAE: 0.3047\n",
      "Epoch 0790 | Loss: 0.1798 | MAE: 0.3015\n",
      "Epoch 0800 | Loss: 0.1764 | MAE: 0.2983\n",
      "Epoch 0810 | Loss: 0.1731 | MAE: 0.2952\n",
      "Epoch 0820 | Loss: 0.1698 | MAE: 0.2921\n",
      "Epoch 0830 | Loss: 0.1667 | MAE: 0.2890\n",
      "Epoch 0840 | Loss: 0.1636 | MAE: 0.2861\n",
      "Epoch 0850 | Loss: 0.1606 | MAE: 0.2832\n",
      "Epoch 0860 | Loss: 0.1576 | MAE: 0.2803\n",
      "Epoch 0870 | Loss: 0.1547 | MAE: 0.2774\n",
      "Epoch 0880 | Loss: 0.1518 | MAE: 0.2746\n",
      "Epoch 0890 | Loss: 0.1490 | MAE: 0.2719\n",
      "Epoch 0900 | Loss: 0.1463 | MAE: 0.2691\n",
      "Epoch 0910 | Loss: 0.1436 | MAE: 0.2664\n",
      "Epoch 0920 | Loss: 0.1409 | MAE: 0.2637\n",
      "Epoch 0930 | Loss: 0.1384 | MAE: 0.2611\n",
      "Epoch 0940 | Loss: 0.1358 | MAE: 0.2586\n",
      "Epoch 0950 | Loss: 0.1333 | MAE: 0.2561\n",
      "Epoch 0960 | Loss: 0.1309 | MAE: 0.2536\n",
      "Epoch 0970 | Loss: 0.1285 | MAE: 0.2512\n",
      "Epoch 0980 | Loss: 0.1262 | MAE: 0.2488\n",
      "Epoch 0990 | Loss: 0.1239 | MAE: 0.2465\n",
      "Epoch 1000 | Loss: 0.1217 | MAE: 0.2442\n",
      "Epoch 1010 | Loss: 0.1195 | MAE: 0.2419\n",
      "Epoch 1020 | Loss: 0.1174 | MAE: 0.2397\n",
      "Epoch 1030 | Loss: 0.1154 | MAE: 0.2375\n",
      "Epoch 1040 | Loss: 0.1134 | MAE: 0.2353\n",
      "Epoch 1050 | Loss: 0.1114 | MAE: 0.2332\n",
      "Epoch 1060 | Loss: 0.1095 | MAE: 0.2310\n",
      "Epoch 1070 | Loss: 0.1076 | MAE: 0.2288\n",
      "Epoch 1080 | Loss: 0.1057 | MAE: 0.2267\n",
      "Epoch 1090 | Loss: 0.1039 | MAE: 0.2246\n",
      "Epoch 1100 | Loss: 0.1021 | MAE: 0.2225\n",
      "Epoch 1110 | Loss: 0.1004 | MAE: 0.2204\n",
      "Epoch 1120 | Loss: 0.0987 | MAE: 0.2184\n",
      "Epoch 1130 | Loss: 0.0970 | MAE: 0.2164\n",
      "Epoch 1140 | Loss: 0.0954 | MAE: 0.2144\n",
      "Epoch 1150 | Loss: 0.0938 | MAE: 0.2124\n",
      "Epoch 1160 | Loss: 0.0923 | MAE: 0.2104\n",
      "Epoch 1170 | Loss: 0.0907 | MAE: 0.2085\n",
      "Epoch 1180 | Loss: 0.0892 | MAE: 0.2066\n",
      "Epoch 1190 | Loss: 0.0878 | MAE: 0.2046\n",
      "Epoch 1200 | Loss: 0.0863 | MAE: 0.2027\n",
      "Epoch 1210 | Loss: 0.0849 | MAE: 0.2009\n",
      "Epoch 1220 | Loss: 0.0836 | MAE: 0.1990\n",
      "Epoch 1230 | Loss: 0.0822 | MAE: 0.1971\n",
      "Epoch 1240 | Loss: 0.0809 | MAE: 0.1953\n",
      "Epoch 1250 | Loss: 0.0796 | MAE: 0.1935\n",
      "Epoch 1260 | Loss: 0.0783 | MAE: 0.1918\n",
      "Epoch 1270 | Loss: 0.0771 | MAE: 0.1900\n",
      "Epoch 1280 | Loss: 0.0759 | MAE: 0.1883\n",
      "Epoch 1290 | Loss: 0.0747 | MAE: 0.1866\n",
      "Epoch 1300 | Loss: 0.0735 | MAE: 0.1849\n",
      "Epoch 1310 | Loss: 0.0724 | MAE: 0.1833\n",
      "Epoch 1320 | Loss: 0.0713 | MAE: 0.1816\n",
      "Epoch 1330 | Loss: 0.0702 | MAE: 0.1800\n",
      "Epoch 1340 | Loss: 0.0691 | MAE: 0.1784\n",
      "Epoch 1350 | Loss: 0.0681 | MAE: 0.1768\n",
      "Epoch 1360 | Loss: 0.0671 | MAE: 0.1752\n",
      "Epoch 1370 | Loss: 0.0661 | MAE: 0.1737\n",
      "Epoch 1380 | Loss: 0.0651 | MAE: 0.1721\n",
      "Epoch 1390 | Loss: 0.0642 | MAE: 0.1706\n",
      "Epoch 1400 | Loss: 0.0633 | MAE: 0.1691\n",
      "Epoch 1410 | Loss: 0.0624 | MAE: 0.1676\n",
      "Epoch 1420 | Loss: 0.0615 | MAE: 0.1660\n",
      "Epoch 1430 | Loss: 0.0606 | MAE: 0.1646\n",
      "Epoch 1440 | Loss: 0.0597 | MAE: 0.1631\n",
      "Epoch 1450 | Loss: 0.0589 | MAE: 0.1616\n",
      "Epoch 1460 | Loss: 0.0581 | MAE: 0.1602\n",
      "Epoch 1470 | Loss: 0.0573 | MAE: 0.1588\n",
      "Epoch 1480 | Loss: 0.0565 | MAE: 0.1574\n",
      "Epoch 1490 | Loss: 0.0557 | MAE: 0.1560\n",
      "Epoch 1500 | Loss: 0.0550 | MAE: 0.1547\n",
      "Epoch 1510 | Loss: 0.0542 | MAE: 0.1533\n",
      "Epoch 1520 | Loss: 0.0535 | MAE: 0.1520\n",
      "Epoch 1530 | Loss: 0.0528 | MAE: 0.1507\n",
      "Epoch 1540 | Loss: 0.0521 | MAE: 0.1494\n",
      "Epoch 1550 | Loss: 0.0514 | MAE: 0.1481\n",
      "Epoch 1560 | Loss: 0.0507 | MAE: 0.1469\n",
      "Epoch 1570 | Loss: 0.0501 | MAE: 0.1456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 0\n",
    "previous_loss = float('inf')  \n",
    "loss_value = 0.5  \n",
    "convergence_threshold = 0.05 \n",
    "\n",
    "while loss_value > convergence_threshold:\n",
    "    epoch += 1\n",
    "    \n",
    "    predictions = model.forward(X_train)\n",
    "    \n",
    "    loss_value = loss.calculate(y_train.values.reshape(-1, 1), predictions)\n",
    "    \n",
    "    model.backward(y_train.values.reshape(-1, 1), predictions)\n",
    "    \n",
    "    model.update()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        mean_absolute_error = np.mean(np.abs(y_train.values - predictions.flatten()))\n",
    "        print(f'Epoch {epoch:04d} | Loss: {loss_value:.4f} | MAE: {mean_absolute_error:.4f}')\n",
    "    \n",
    "    if abs(previous_loss - loss_value) < 1e-6:\n",
    "        print(\"Converged: Loss improvement below threshold.\")\n",
    "        break\n",
    "    \n",
    "    previous_loss = loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.7864, MAE: 0.6867, MSE: 0.7864, RMSE: 0.8868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred_test = model.forward(X_test)\n",
    "\n",
    "test_loss = loss.calculate(y_test.values.reshape(-1, 1), y_pred_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_test.flatten())  \n",
    "mse = mean_squared_error(y_test.values, y_pred_test.flatten())\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
